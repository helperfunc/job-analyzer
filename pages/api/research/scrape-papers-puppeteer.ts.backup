import { NextApiRequest, NextApiResponse } from 'next'
import puppeteer from 'puppeteer'
import * as cheerio from 'cheerio'
import { supabase } from '../../../lib/supabase'

interface Paper {
  title: string
  authors: string[]
  publication_date?: string
  abstract?: string
  url: string
  arxiv_id?: string
  github_url?: string
  company: string
  tags: string[]
}

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  if (req.method !== 'POST') {
    return res.status(405).json({
      success: false,
      error: 'Method not allowed'
    })
  }

  const { company } = req.body

  if (!company || !['openai', 'anthropic', 'deepmind'].includes(company.toLowerCase())) {
    return res.status(400).json({
      success: false,
      error: 'Company must be either "openai", "anthropic", or "deepmind"'
    })
  }

  let browser = null

  try {
    console.log(`üöÄ Starting Puppeteer to scrape ${company} research papers...`)
    
    // Start scraping asynchronously and return immediately
    const scrapingLogic = async () => {
      try {
        // Launch Puppeteer with settings to bypass Cloudflare
        browser = await puppeteer.launch({
          headless: 'new', // Using new headless mode
          args: [
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-dev-shm-usage',
            '--disable-accelerated-2d-canvas',
            '--disable-gpu',
            '--window-size=1920x1080',
            '--disable-blink-features=AutomationControlled',
            '--disable-features=VizDisplayCompositor'
          ]
        })
        
        const page = await browser.newPage()
        
        // Set a realistic user agent with English language preference
        await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        // Set viewport
        await page.setViewport({ width: 1920, height: 1080 })
        
        // Set extra headers to request English content
        await page.setExtraHTTPHeaders({
          'Accept-Language': 'en-US,en;q=0.9',
          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'
        })
        
        // Hide webdriver presence
        await page.evaluateOnNewDocument(() => {
          Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined,
          })
          Object.defineProperty(navigator, 'languages', {
            get: () => ['en-US', 'en'],
          })
          Object.defineProperty(navigator, 'language', {
            get: () => 'en-US',
          })
        })

        const papers: Paper[] = []

        if (company.toLowerCase() === 'openai') {
          await scrapeOpenAIWithPuppeteer(page, papers)
        } else if (company.toLowerCase() === 'anthropic') {
          await scrapeAnthropicWithPuppeteer(page, papers)
        } else if (company.toLowerCase() === 'deepmind') {
          await scrapeDeepMindWithPuppeteer(page, papers)
        }

        // Save papers to database if Supabase is configured
        if (supabase && papers.length > 0) {
          console.log(`üíæ Attempting to save ${papers.length} papers to database...`)
          let savedCount = 0
          
          for (let i = 0; i < papers.length; i++) {
            const paper = papers[i]
            try {
              console.log(`üíæ Saving paper ${i + 1}/${papers.length}: ${paper.title.substring(0, 50)}...`)
              
              const { data, error } = await supabase
                .from('research_papers')
                .upsert([paper], { 
                  onConflict: 'url'
                })
                .select()

              if (error) {
                console.error(`‚ùå Error saving paper "${paper.title}":`, error)
              } else if (data) {
                savedCount += data.length
                console.log(`‚úÖ Successfully saved: ${paper.title.substring(0, 50)}...`)
              }
            } catch (err) {
              console.error(`‚ùå Exception saving paper "${paper.title}":`, err)
            }
          }
          console.log(`‚úÖ Total saved ${savedCount} out of ${papers.length} papers to database`)
        }

        return {
          success: true,
          company,
          count: papers.length,
          papers: papers.slice(0, 10), // Return first 10 papers as preview
          message: `Successfully scraped ${papers.length} papers from ${company} using Puppeteer`
        }
      } catch (scrapingError) {
        console.error(`‚ùå Background research scraping failed for ${company}:`, scrapingError)
        return {
          success: false,
          company,
          error: scrapingError instanceof Error ? scrapingError.message : 'Unknown scraping error'
        }
      } finally {
        if (browser) {
          console.log('üîí Closing browser...')
          await browser.close()
        }
      }
    }

    // Create a timeout promise
    const timeoutPromise = new Promise((_, reject) => {
      setTimeout(() => reject(new Error('Scraping timeout')), 10 * 60 * 1000) // 10 minute timeout
    })

    // Start scraping in background and return immediately
    Promise.race([scrapingLogic(), timeoutPromise])
      .then((result) => {
        console.log(`‚úÖ Background research scraping completed for ${company}`)
      })
      .catch((error) => {
        console.error(`‚ùå Background research scraping failed for ${company}:`, error)
      })

    // Return immediately to avoid browser timeout
    res.status(200).json({
      success: true,
      message: `Started background scraping for ${company} papers. Use polling to check completion.`,
      company: company,
      status: 'started'
    })

  } catch (error) {
    console.error('Error starting background research scraping:', error)
    res.status(500).json({
      success: false,
      error: 'Failed to start background scraping',
      details: error instanceof Error ? error.message : 'Unknown error'
    })
  }
}

async function scrapeOpenAIWithPuppeteer(page: any, papers: Paper[]): Promise<void> {
  try {
    const url = 'https://openai.com/research/index/'
    console.log(`üöÄ Navigating to ${url}...`)
    
    await page.goto(url, { 
      waitUntil: 'networkidle2', 
      timeout: 60000 
    })
    
    // Check if we got redirected to a localized version and redirect to English
    const currentUrl = page.url()
    if (currentUrl.includes('/zh-Hans') || currentUrl.includes('/zh-CN')) {
      console.log('üåç Detected localized page, redirecting to English version...')
      const englishUrl = currentUrl.replace(/\/zh-Hans(-CN)?/, '')
      await page.goto(englishUrl, { 
        waitUntil: 'networkidle2', 
        timeout: 60000 
      })
    }
    
    // Wait for initial content to load and bypass Cloudflare
    await new Promise(resolve => setTimeout(resolve, 8000))
    
    // Continuously click "Load more" button until all content is loaded (back to 2016)
    let loadMoreAttempts = 0
    let consecutiveFailures = 0
    const maxConsecutiveFailures = 3 // Stop after 3 consecutive failures
    const maxTotalAttempts = 50 // Reasonable limit to avoid infinite loop
    
    console.log(`üîÑ Starting to load all papers by clicking "Load more" repeatedly...`)
    
    while (loadMoreAttempts < maxTotalAttempts && consecutiveFailures < maxConsecutiveFailures) {
      try {
        loadMoreAttempts++
        console.log(`üîÑ Attempt ${loadMoreAttempts}: Looking for "Load more" button...`)
        
        // Wait for the page to settle between attempts
        await new Promise(resolve => setTimeout(resolve, 3000))
        
        // Get current page content length to detect if new content was loaded
        const beforeContentLength = await page.evaluate(() => document.body.textContent?.length || 0)
        
        // Find and click the "Load more" button
        const loadMoreClicked = await page.evaluate(() => {
          // Find all buttons and look for "Load more"
          const allButtons = Array.from(document.querySelectorAll('button'));
          const loadMoreButton = allButtons.find(btn => {
            const text = btn.textContent?.trim();
            return text === 'Load more';
          });
          
          if (loadMoreButton) {
            // Check if the button is visible and enabled
            const isVisible = loadMoreButton.offsetWidth > 0 && loadMoreButton.offsetHeight > 0;
            const isEnabled = !loadMoreButton.disabled;
            
            if (isVisible && isEnabled) {
              loadMoreButton.click();
              return true;
            } else {
              return false;
            }
          }
          
          return false;
        });
        
        if (loadMoreClicked) {
          console.log(`üì• Successfully clicked "Load more" button (attempt ${loadMoreAttempts})`)
          
          // Wait longer for new content to load
          await new Promise(resolve => setTimeout(resolve, 5000))
          
          // Check if new content was actually loaded
          const afterContentLength = await page.evaluate(() => document.body.textContent?.length || 0)
          const contentIncreased = afterContentLength > beforeContentLength
          
          if (contentIncreased) {
            console.log(`‚úÖ New content loaded! Page size: ${beforeContentLength} ‚Üí ${afterContentLength}`)
            consecutiveFailures = 0 // Reset failure counter
            
            // Check if we've reached 2016 content (optional early termination check)
            const has2016Content = await page.evaluate(() => {
              return document.body.textContent?.includes('2016') || false;
            });
            
            if (has2016Content) {
              console.log(`üéØ Detected 2016 content, likely reached the end of available papers`)
            }
          } else {
            console.log(`‚ö†Ô∏è Button clicked but no new content detected`)
            consecutiveFailures++
          }
        } else {
          console.log(`‚ùå "Load more" button not found or not clickable (attempt ${loadMoreAttempts})`)
          consecutiveFailures++
        }
        
        // Log progress every 5 attempts
        if (loadMoreAttempts % 5 === 0) {
          const currentContentLength = await page.evaluate(() => document.body.textContent?.length || 0)
          console.log(`üìä Progress: ${loadMoreAttempts} attempts, content length: ${currentContentLength}`)
        }
        
      } catch (e) {
        console.log(`‚ö†Ô∏è Error on attempt ${loadMoreAttempts}:`, e)
        consecutiveFailures++
      }
    }
    
    if (consecutiveFailures >= maxConsecutiveFailures) {
      console.log(`üèÅ Stopped after ${consecutiveFailures} consecutive failures to load more content`)
    } else if (loadMoreAttempts >= maxTotalAttempts) {
      console.log(`üèÅ Stopped after reaching maximum attempts (${maxTotalAttempts})`)
    }
    
    console.log(`üìã Final stats: ${loadMoreAttempts} total attempts to click "Load more"`)
    
    // Get the final HTML after loading all content
    const html = await page.content()
    
    // Check for Cloudflare protection
    if (html.includes('Just a moment') || html.includes('Please wait')) {
      console.log(`‚ö†Ô∏è Cloudflare challenge detected, trying longer wait...`)
      await new Promise(resolve => setTimeout(resolve, 15000))
      const retryHtml = await page.content()
      if (retryHtml.includes('Just a moment')) {
        console.log(`‚ùå Could not bypass Cloudflare`)
        return
      }
    }
    
    console.log(`üìä Final page content length: ${html.length}`)
    
    const $ = cheerio.load(html)
    
    // Enhanced parsing based on the actual OpenAI research page structure
    console.log('üîç Parsing OpenAI research papers using targeted title extraction...')
    
    const bodyText = $('body').text()
    console.log(`üìä Page content sample: ${bodyText.substring(0, 500)}...`)
    
    // Use targeted exact title matching to ensure clean titles
    console.log('üîÑ Using precise title extraction...')
    
    const foundTitles = new Set<string>()
    
    // Define exact, clean titles based on OpenAI research history (2025 back to 2016)
    const knownTitles = [
      // 2025 papers
      'GPT-5 System Card',
      'Introducing GPT-5', 
      'From hard refusals to safe-completions: toward output-centric safety training',
      'Introducing gpt-oss',
      'gpt-oss-120b & gpt-oss-20b Model Card',
      'Estimating worst case frontier risks of open weight LLMs',
      'Pioneering an AI clinical copilot with Penda Health',
      'ChatGPT agent System Card',
      'Introducing ChatGPT agent',
      'Toward understanding and preventing misalignment generalization',
      'Introducing HealthBench',
      'Introducing OpenAI o3 and o4-mini',
      'Thinking with images',
      'OpenAI o3 and o4-mini System Card',
      'Our updated Preparedness Framework',
      'Introducing GPT-4.1 in the API',
      'BrowseComp: a benchmark for browsing agents',
      'PaperBench: Evaluating AI\'s Ability to Replicate AI Research',
      'Introducing 4o Image Generation',
      'Addendum to GPT-4o System Card: 4o image generation',
      'Early methods for studying affective use and emotional well-being on ChatGPT',
      'Introducing next-generation audio models in the API',
      'Detecting misbehavior in frontier reasoning models',
      'OpenAI GPT-4.5 System Card',
      'Introducing GPT-4.5',
      'Deep research System Card',
      'Introducing the SWE-Lancer benchmark',
      'Sharing the latest Model Spec',
      'Introducing deep research',
      'OpenAI o3-mini',
      'OpenAI o3-mini System Card',
      'Operator System Card',
      'Computer-Using Agent',
      'Trading inference-time compute for adversarial robustness',
      
      // 2024 papers
      'Deliberative alignment: reasoning enables safer language models',
      'Sora System Card',
      'OpenAI o1 System Card',
      'Advancing red teaming with people and AI',
      'Introducing SimpleQA',
      'Simplifying, stabilizing, and scaling continuous-time consistency models',
      'Evaluating fairness in ChatGPT',
      'MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering',
      'Learning to reason with LLMs',
      'OpenAI o1-mini',
      'Introducing SWE-bench Verified',
      'GPT-4o System Card',
      'Improving Model Safety Behavior with Rule-Based Rewards',
      'GPT-4o mini: advancing cost-efficient intelligence',
      'Prover-Verifier Games improve legibility of language model outputs',
      'OpenAI and Los Alamos National Laboratory announce research partnership',
      'Finding GPT-4\'s mistakes with GPT-4',
      'Consistency Models',
      'Improved Techniques for Training Consistency Models',
      'A Holistic Approach to Undesired Content Detection in the Real World',
      'Extracting Concepts from GPT-4',
      'Hello GPT-4o',
      'Understanding the source of what we see and hear online',
      'The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions',
      'Video generation models as world simulators',
      'Building an early warning system for LLM-aided biological threat creation',
      
      // 2023 papers  
      'Practices for Governing Agentic AI Systems',
      'DALL¬∑E 3 system card',
      'GPT-4V(ision) system card',
      'Confidence-Building Measures for Artificial Intelligence: Workshop proceedings',
      'Frontier AI regulation: Managing emerging risks to public safety',
      'Improving mathematical reasoning with process supervision',
      'Democratic inputs to AI',
      'Language models can explain neurons in language models',
      'GPTs are GPTs: An early look at the labor market impact potential of large language models',
      'GPT-4',
      'Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk',
      
      // 2022 papers
      'Point-E: A system for generating 3D point clouds from complex prompts',
      'Scaling laws for reward model overoptimization',
      'Introducing Whisper',
      'Efficient training of language models to fill in the middle',
      'A hazard analysis framework for code synthesis large language models',
      'DALL¬∑E 2 pre-training mitigations',
      'Learning to play Minecraft with Video PreTraining',
      'Evolution through large models',
      'AI-written critiques help humans notice flaws',
      'Techniques for training large neural networks',
      'Teaching models to express their uncertainty in words',
      'Hierarchical text-conditional image generation with CLIP latents',
      'Measuring Goodhart\'s law',
      'A research agenda for assessing the economic impacts of code generation models',
      'Lessons learned on language model safety and misuse',
      'Solving (some) formal math olympiad problems',
      'Aligning language models to follow instructions',
      'Text and code embeddings by contrastive pre-training',
      'WebGPT: Improving the factual accuracy of language models through web browsing',
      'Solving math word problems',
      'Summarizing books with human feedback',
      'TruthfulQA: Measuring how models mimic human falsehoods',
      'Introducing Triton: Open-source GPU programming for neural networks',
      'Evaluating large language models trained on code',
      'Improving language model behavior by training on a curated dataset',
      'Multimodal neurons in artificial neural networks',
      'Understanding the capabilities, limitations, and societal impact of large language models',
      'Scaling Kubernetes to 7,500 nodes',
      'DALL¬∑E: Creating images from text',
      'CLIP: Connecting text and images',
      
      // More 2020-2016 papers (expanding the list)
      'Generative language modeling for automated theorem proving',
      'Learning to summarize with human feedback',
      'Image GPT',
      'Language models are few-shot learners',
      'AI and efficiency',
      'Jukebox',
      'Improving verifiability in AI development',
      'OpenAI Microscope',
      'Scaling laws for neural language models',
      'Dota 2 with large scale deep reinforcement learning',
      'Deep double descent',
      'Procgen Benchmark',
      'Safety Gym',
      'Benchmarking safe exploration in deep reinforcement learning',
      'GPT-2: 1.5B release',
      'Solving Rubik\'s Cube with a robot hand',
      'Fine-tuning GPT-2 from human preferences',
      'Emergent tool use from multi-agent interaction',
      'Testing robustness against unforeseen adversaries',
      'GPT-2: 6-month follow-up',
      'Why responsible AI development needs cooperation on safety',
      'Transfer of adversarial robustness between perturbation types',
      'MuseNet',
      'Generative modeling with sparse transformers',
      'OpenAI Five defeats Dota 2 world champions',
      'Better language models and their implications'
    ]
    
    // Look for each known title in the text
    for (const title of knownTitles) {
      if (bodyText.includes(title) && !foundTitles.has(title)) {
        foundTitles.add(title)
        
        // Extract surrounding text for context
        const titleIndex = bodyText.indexOf(title)
        const beforeTitle = bodyText.substring(Math.max(0, titleIndex - 300), titleIndex)
        const afterTitle = bodyText.substring(titleIndex + title.length, titleIndex + title.length + 600)
        
        // Extract publication date with improved pattern matching
        let publicationDate = '2025-08-07' // Default recent date
        const datePatterns = [
          /(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{1,2}),?\s+(202[0-9])/i,
          /(202[0-9])-(0[1-9]|1[0-2])-(0[1-9]|[12][0-9]|3[01])/
        ]
        
        for (const pattern of datePatterns) {
          const dateMatch = beforeTitle.match(pattern)
          if (dateMatch) {
            try {
              const dateStr = dateMatch[0]
              const date = new Date(dateStr)
              if (!isNaN(date.getTime())) {
                publicationDate = date.toISOString().split('T')[0]
                break
              }
            } catch (e) {
              // Continue to next pattern
            }
          }
        }
        
        // Extract paper type
        let paperType = 'Publication'
        const typeMatch = beforeTitle.match(/(Publication|Release|Safety|Milestone|Conclusion|Product|Research)(?=.*?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))/i)
        if (typeMatch) {
          paperType = typeMatch[1]
        }
        
        // Extract clean abstract
        let abstract = afterTitle.trim()
        // Remove leading non-alphabetic characters
        abstract = abstract.replace(/^[^A-Za-z]*/, '').trim()
        
        // Find the first sentence or meaningful chunk
        if (abstract.length > 0) {
          // Look for sentence endings
          const sentenceEnd = abstract.search(/[.!?]\s+[A-Z]/)
          if (sentenceEnd > 20) {
            abstract = abstract.substring(0, sentenceEnd + 1)
          } else {
            // Take first reasonable chunk up to 300 chars
            abstract = abstract.substring(0, 300)
            if (!abstract.endsWith('.') && !abstract.endsWith('...')) {
              abstract += '...'
            }
          }
        } else {
          abstract = `${paperType} from OpenAI focusing on ${title.toLowerCase()}.`
        }
        
        // Generate proper URL for this paper based on title
        // Use known URL mappings for accuracy, fall back to generated URLs
        const knownUrlMappings: Record<string, string> = {
          'gpt-oss-120b & gpt-oss-20b Model Card': 'https://openai.com/index/gpt-oss-model-card/',
          'Introducing gpt-oss': 'https://openai.com/index/introducing-gpt-oss/',
          'Introducing GPT-4.1 in the API': 'https://openai.com/index/introducing-gpt-4-1-in-the-api/',
          'Introducing GPT-5': 'https://openai.com/index/introducing-gpt-5/',
          'Introducing ChatGPT agent': 'https://openai.com/index/introducing-chatgpt-agent/',
          'Introducing OpenAI o3 and o4-mini': 'https://openai.com/index/introducing-openai-o3-and-o4-mini/',
          'Introducing GPT-4.5': 'https://openai.com/index/introducing-gpt-4-5/',
          'Introducing deep research': 'https://openai.com/index/introducing-deep-research/',
          'Computer-Using Agent': 'https://openai.com/index/computer-using-agent/',
          'Hello GPT-4o': 'https://openai.com/index/hello-gpt-4o/',
          'GPT-4o mini: advancing cost-efficient intelligence': 'https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/',
          'Video generation models as world simulators': 'https://openai.com/index/video-generation-models-as-world-simulators/',
          'Introducing Whisper': 'https://openai.com/index/introducing-whisper/',
          'Learning to reason with LLMs': 'https://openai.com/index/learning-to-reason-with-llms/',
          'GPT-4': 'https://openai.com/index/gpt-4-research/',
          'Language models are few-shot learners': 'https://openai.com/index/language-models-are-few-shot-learners/'
        }
        
        let paperUrl = knownUrlMappings[title]
        
        if (!paperUrl) {
          // Generate URL from title as fallback
          let urlSlug = title.toLowerCase()
            .replace(/\s*&\s*/g, '-and-') // Convert & to -and-
            .replace(/[^\w\s-]/g, '') // Remove special chars except spaces and hyphens
            .replace(/\s+/g, '-') // Convert spaces to hyphens
            .replace(/-+/g, '-') // Collapse multiple hyphens
            .replace(/^-|-$/g, '') // Remove leading/trailing hyphens
            .slice(0, 60) // Slightly longer limit for better accuracy
          
          paperUrl = `https://openai.com/index/${urlSlug}/`
        }
        
        papers.push({
          title, // Use the exact, clean title
          authors: ['OpenAI'],
          publication_date: publicationDate,
          abstract,
          url: paperUrl,
          company: 'OpenAI',
          tags: generateTags(title + ' ' + abstract + ' ' + paperType)
        })
        
        console.log(`üìù Found paper: "${title}" (${publicationDate})`)
      }
    }
    
    console.log(`‚úÖ Found ${papers.length} papers total from OpenAI`)
    
  } catch (error) {
    console.error('Error scraping OpenAI with Puppeteer:', error)
  }
}

async function scrapeAnthropicWithPuppeteer(page: any, papers: Paper[]): Promise<void> {
  try {
    // Focus on the research page which should contain publications
    const urls = [
      'https://www.anthropic.com/research'
    ]
    
    for (const url of urls) {
      try {
        console.log(`üìÑ Navigating to ${url}...`)
        
        await page.goto(url, { 
          waitUntil: 'networkidle2', 
          timeout: 60000 
        })
        
        // Wait for content to load
        await new Promise(resolve => setTimeout(resolve, 8000))
        
        const html = await page.content()
        
        // Check if we got past any protection
        if (html.includes('Just a moment') || html.includes('Please wait')) {
          console.log(`‚ö†Ô∏è Protection detected on ${url}, trying longer wait...`)
          await new Promise(resolve => setTimeout(resolve, 15000))
        }
        
        console.log(`üìä Page loaded, content length: ${html.length}`)
        
        const $ = cheerio.load(html)
        
        // Look for research papers - focus on publication section
        const selectors = [
          // Look for publication containers
          'article',
          '[class*="publication"]',
          '[class*="paper"]', 
          '[class*="research"]',
          '[class*="card"]',
          '[class*="item"]',
          
          // Specific Anthropic content
          'div[data-testid*="publication"]',
          'div[data-testid*="paper"]',
          
          // Headings and links
          'h1, h2, h3, h4, h5',
          'a[href*="/research/"]',
        ]
        
        const foundTitles = new Set<string>()
        
        for (const selector of selectors) {
          $(selector).each((_, element) => {
            const $elem = $(element)
            let title = ''
            let paperUrl = ''
            
            // Extract title
            if ($elem.is('a')) {
              title = $elem.text().trim()
              paperUrl = $elem.attr('href') || ''
            } else {
              title = $elem.text().trim()
              paperUrl = $elem.find('a').first().attr('href') || 
                        $elem.closest('a').attr('href') || ''
            }
            
            // Check if this looks like a research paper for Anthropic
            if (title && 
                title.length > 5 && 
                title.length < 300 &&
                !foundTitles.has(title) &&
                // Exclude navigation elements
                !(/^(Research|Publications|News|About|API)$/i.test(title)) &&
                (
                  // Match Anthropic-specific models and research
                  /\b(Claude|Constitutional|Haiku|Sonnet|Opus)\b/i.test(title) ||
                  // Match research topics
                  /\b(Safety|Alignment|RLHF|Interpretability|Oversight)\b/i.test(title) ||
                  // Match paper-style titles
                  /\b(Training|Scaling|Measuring|Mapping|Many-Shot)\b/i.test(title) ||
                  // Research-related terms
                  /\b(paper|research|model|system|card|feedback)\b/i.test(title) ||
                  // Contains year
                  /202[0-9]/.test(title)
                )) {
              
              foundTitles.add(title)
              
              // Normalize URL
              if (paperUrl && !paperUrl.startsWith('http')) {
                paperUrl = `https://www.anthropic.com${paperUrl.startsWith('/') ? '' : '/'}${paperUrl}`
              } else if (!paperUrl) {
                // Generate URL from title as fallback (better than generic /research)
                const urlSlug = title.toLowerCase()
                  .replace(/[^\w\s-]/g, '') // Remove special chars except spaces and hyphens
                  .replace(/\s+/g, '-') // Convert spaces to hyphens
                  .replace(/-+/g, '-') // Collapse multiple hyphens
                  .replace(/^-|-$/g, '') // Remove leading/trailing hyphens
                  .slice(0, 60) // Limit length
                
                paperUrl = `https://www.anthropic.com/research/${urlSlug}`
              }
              
              // Extract additional info
              const contextText = $elem.parent().text() + ' ' + $elem.siblings().text()
              
              // Extract date
              const dateMatch = contextText.match(/\b(202[0-9]|201[0-9])-(\d{2})-(\d{2})\b/) ||
                               contextText.match(/\b(202[0-9]|201[0-9])\b/)
              
              const publication_date = dateMatch ? 
                (dateMatch[0].length === 4 ? `${dateMatch[0]}-01-01` : dateMatch[0]) :
                new Date().toISOString().split('T')[0]
              
              // Extract abstract
              const abstract = $elem.siblings('p').first().text().trim() ||
                              $elem.parent().find('p').first().text().trim() ||
                              contextText.substring(0, 300) + '...'
              
              papers.push({
                title,
                authors: ['Anthropic'],
                publication_date,
                abstract,
                url: paperUrl,
                company: 'Anthropic',
                tags: generateTags(title + ' ' + abstract)
              })
              
              console.log(`üìù Found paper: ${title}`)
            }
          })
        }
        
        if (papers.length > 0) {
          console.log(`‚úÖ Found ${papers.length} papers from ${url}`)
          break
        }
        
      } catch (pageError) {
        console.log(`‚ö†Ô∏è Error loading ${url}:`, pageError)
        continue
      }
    }
    
  } catch (error) {
    console.error('Error scraping Anthropic with Puppeteer:', error)
  }
}

async function scrapeDeepMindWithPuppeteer(page: any, papers: Paper[]): Promise<void> {
  try {
    console.log('üöÄ Starting DeepMind papers scraping...')
    
    const url = 'https://deepmind.google/research/publications/'
    console.log(`üìÑ Navigating to ${url}...`)
    
    await page.goto(url, { 
      waitUntil: 'networkidle2', 
      timeout: 60000 
    })
    
    // Wait for content to load
    await new Promise(resolve => setTimeout(resolve, 8000))
    
    // Wait for specific elements that indicate papers are loaded
    try {
      await page.waitForSelector('a[href*="/research/publications/"]', { timeout: 10000 })
      console.log('‚úÖ Found publication links on page')
      
      // Check if content is dynamically loaded
      const linkCount = await page.evaluate(() => {
        return document.querySelectorAll('a[href*="/research/publications/"]').length
      })
      console.log(`üìä Found ${linkCount} publication links via JavaScript`)
      
      // Try to wait for specific content
      await page.waitForFunction(
        () => {
          const links = document.querySelectorAll('a[href*="/research/publications/"]')
          return links.length > 5 // Wait until we have at least 5 paper links
        },
        { timeout: 15000 }
      )
    } catch (e) {
      console.log('‚ö†Ô∏è Timeout waiting for papers to load:', e.message)
    }
    
    // Take a screenshot for debugging (optional)
    // await page.screenshot({ path: 'deepmind-page.png' })
    
    // Log page title to verify we're on the right page
    const pageTitle = await page.title()
    console.log(`üìÑ Page title: "${pageTitle}"`)
    
    // Try to load more papers by scrolling and clicking "Load more" if available
    let loadMoreAttempts = 0
    const maxAttempts = 15 // Increased for 12 pages
    
    console.log(`üîÑ Looking for more papers across multiple pages...`)
    
    while (loadMoreAttempts < maxAttempts) {
      try {
        loadMoreAttempts++
        
        // Scroll to bottom to trigger lazy loading
        await page.evaluate(() => {
          window.scrollTo(0, document.body.scrollHeight)
        })
        
        await new Promise(resolve => setTimeout(resolve, 3000)) // Longer wait
        
        // Look for pagination buttons, load more buttons, and next page links
        const loadMoreClicked = await page.evaluate(() => {
          // Look for various types of pagination controls
          const buttons = Array.from(document.querySelectorAll('button, a, [role="button"]'))
          
          // Try to find load more button
          let loadMoreButton = buttons.find(btn => {
            const text = btn.textContent?.toLowerCase() || ''
            const ariaLabel = btn.getAttribute('aria-label')?.toLowerCase() || ''
            return text.includes('load more') || 
                   text.includes('show more') || 
                   text.includes('view more') ||
                   text.includes('see more') ||
                   ariaLabel.includes('load more') ||
                   ariaLabel.includes('next')
          })
          
          // If no load more button, try pagination controls
          if (!loadMoreButton) {
            loadMoreButton = buttons.find(btn => {
              const text = btn.textContent?.toLowerCase().trim() || ''
              const ariaLabel = btn.getAttribute('aria-label')?.toLowerCase() || ''
              return text === 'next' || 
                     text.includes('‚Üí') || 
                     text.includes('>') ||
                     ariaLabel.includes('next page') ||
                     (text.match(/^\d+$/) && parseInt(text) > 1) // Page numbers > 1
            })
          }
          
          // Try clicking any button with "next" or pagination indicators
          if (!loadMoreButton) {
            loadMoreButton = buttons.find(btn => {
              const classList = btn.className.toLowerCase()
              return classList.includes('next') || 
                     classList.includes('pagination') ||
                     classList.includes('load') ||
                     classList.includes('more')
            })
          }
          
          if (loadMoreButton && loadMoreButton instanceof HTMLElement) {
            // Check if button is visible and enabled
            const isVisible = loadMoreButton.offsetWidth > 0 && loadMoreButton.offsetHeight > 0
            const isEnabled = !loadMoreButton.hasAttribute('disabled') && 
                             !loadMoreButton.classList.contains('disabled')
            
            if (isVisible && isEnabled) {
              loadMoreButton.click()
              return true
            }
          }
          return false
        })
        
        if (loadMoreClicked) {
          console.log(`üì• Clicked pagination/load more (attempt ${loadMoreAttempts})`)
          await new Promise(resolve => setTimeout(resolve, 4000)) // Longer wait for page load
          
          // Check if new content was loaded
          const newContentLength = await page.evaluate(() => document.body.textContent?.length || 0)
          console.log(`üìä Page content length after load: ${newContentLength}`)
        } else {
          console.log(`‚ùå No more pagination controls found (attempt ${loadMoreAttempts})`)
          
          // Try one more approach: look for specific DeepMind pagination patterns
          const pageChanged = await page.evaluate(() => {
            // Look for numbered pagination
            const pageLinks = Array.from(document.querySelectorAll('a[href*="page"], a[href*="offset"], button[data-page]'))
            const nextPageLink = pageLinks.find(link => {
              const href = link.getAttribute('href') || ''
              const text = link.textContent?.trim() || ''
              return href.includes('page=2') || href.includes('offset=') || text.match(/^[2-9]|1[0-2]$/)
            })
            
            if (nextPageLink && nextPageLink instanceof HTMLElement) {
              nextPageLink.click()
              return true
            }
            return false
          })
          
          if (!pageChanged) {
            break
          }
          
          await new Promise(resolve => setTimeout(resolve, 4000))
        }
      } catch (e) {
        console.log(`‚ö†Ô∏è Error on attempt ${loadMoreAttempts}:`, e)
        break
      }
    }
    
    console.log(`üîÑ Completed ${loadMoreAttempts} attempts to load more content`)
    
    const html = await page.content()
    console.log(`üìä Page loaded, content length: ${html.length}`)
    
    const $ = cheerio.load(html)
    
    // Look for publication cards/articles on DeepMind's site
    const selectors = [
      // Try more specific selectors for DeepMind's site structure
      'a[href^="/research/publications/"][href*="-"]', // Links to individual papers
      'article.glue-card',
      'div.glue-card',
      '[class*="card"] a[href*="/research/publications/"]',
      'div[class*="publication"]',
      'div[class*="research-card"]',
      'div[class*="paper"]',
      'div[class*="result"]',
      'li[class*="publication"]',
      '[data-testid*="publication"]',
      '.publication-item',
      'article',
      '[role="article"]'
    ]
    
    const foundTitles = new Set<string>()
    const foundUrls = new Set<string>()
    
    // First, let's see what we're working with
    console.log('üîç Debugging DeepMind page structure...')
    
    // Log all links that might be papers
    const allLinks = $('a[href*="/research/publications/"]').toArray()
    console.log(`üìä Found ${allLinks.length} links containing "/research/publications/"`)
    
    if (allLinks.length > 0) {
      console.log('üìã Sample links:')
      allLinks.slice(0, 10).forEach((link) => {
        const href = $(link).attr('href')
        const text = $(link).text().trim()
        console.log(`  - ${href}: "${text.substring(0, 80)}..."`)
      })
    }
    
    // Look for paper titles in the correct elements
    console.log('üîç Looking for paper titles in list-compact__inner elements...')
    
    // First try to find papers using the specific class
    $('.list-compact__inner').each((_, element) => {
      const $elem = $(element)
      const title = $elem.text().trim()
      
      // Find the parent link
      const $link = $elem.closest('a')
      const href = $link.attr('href') || ''
      
      if (title && href.includes('/research/publications/') && !foundTitles.has(title)) {
        foundTitles.add(title)
        
        const paperUrl = href.startsWith('http') ? href : `https://deepmind.google${href}`
        
        // Try to find date from parent elements
        const $parent = $link.closest('article, div, li')
        const parentText = $parent.text()
        
        let publication_date = '2024-01-01'
        const dateMatch = parentText.match(/\b(202[0-9]|201[0-9])\b/)
        if (dateMatch) {
          publication_date = `${dateMatch[0]}-01-01`
        }
        
        papers.push({
          title,
          authors: ['DeepMind'],
          publication_date,
          abstract: 'Research publication from DeepMind.',
          url: paperUrl,
          company: 'DeepMind',
          tags: generateTags(title + ' deepmind')
        })
        
        console.log(`üìù Found paper: "${title}"`)
      }
    })
    
    console.log(`üìä Found ${papers.length} papers via list-compact__inner elements`)
    
    // If no papers found, try a simpler approach - get all links that look like individual papers
    if (papers.length === 0) {
      const paperLinks = allLinks.filter(link => {
        const href = $(link).attr('href') || ''
        const text = $(link).text().trim()
        // Individual paper links usually have slugs after /publications/
        return href.match(/\/research\/publications\/[a-z0-9-]+$/i) && 
               text.length > 10 && 
               text.length < 300 &&
               !text.includes('Research\n') &&
               !text.includes('Publications') &&
               !text.includes('Explore a selection')
      })
    
    console.log(`üìä Found ${paperLinks.length} potential paper links`)
    
    // Process paper links directly
    paperLinks.forEach((link) => {
      const $link = $(link)
      const href = $link.attr('href') || ''
      const title = $link.text().trim()
      
      if (title && !foundTitles.has(title)) {
        foundTitles.add(title)
        
        const paperUrl = href.startsWith('http') ? href : `https://deepmind.google${href}`
        
        // Try to find date and abstract from parent elements
        const $parent = $link.closest('article, div, li')
        const parentText = $parent.text()
        
        let publication_date = '2024-01-01'
        const dateMatch = parentText.match(/\b(202[0-9]|201[0-9])\b/)
        if (dateMatch) {
          publication_date = `${dateMatch[0]}-01-01`
        }
        
        papers.push({
          title,
          authors: ['DeepMind'],
          publication_date,
          abstract: 'Research publication from DeepMind.',
          url: paperUrl,
          company: 'DeepMind',
          tags: generateTags(title + ' deepmind')
        })
        
        console.log(`üìù Found paper via direct link: "${title}"`)
      }
    })
    
    // Try a different approach - look for cards with specific structure
    const cards = $('[class*="card"], article, .glue-card').toArray()
    console.log(`üìä Found ${cards.length} potential card elements`)
    
    // Process cards to find papers
    cards.forEach((card) => {
      const $card = $(card)
      
      // Look for title in various places
      let title = $card.find('h2, h3, h4, [class*="title"], [class*="heading"]').first().text().trim() ||
                 $card.find('a').first().text().trim()
      
      // Look for URL
      let paperUrl = $card.find('a[href*="/research/publications/"]').first().attr('href') ||
                    $card.find('a').first().attr('href') || ''
      
      // Skip if no title or it's a navigation element
      if (!title || title.length < 10 || foundTitles.has(title)) {
        return
      }
      
      // Extract date from card
      const dateText = $card.text()
      let publication_date = '2024-01-01'
      const dateMatch = dateText.match(/\b(202[0-9]|201[0-9])\b/)
      if (dateMatch) {
        publication_date = `${dateMatch[0]}-01-01`
      }
      
      // Extract description/abstract
      const abstract = $card.find('p, [class*="description"], [class*="summary"]').first().text().trim() ||
                      'Research publication from DeepMind.'
      
      if (title && paperUrl && paperUrl.includes('/research/publications/')) {
        // Normalize URL
        if (!paperUrl.startsWith('http')) {
          paperUrl = `https://deepmind.google${paperUrl.startsWith('/') ? '' : '/'}${paperUrl}`
        }
        
        foundTitles.add(title)
        foundUrls.add(paperUrl)
        
        papers.push({
          title,
          authors: ['DeepMind'],
          publication_date,
          abstract: abstract.substring(0, 500) + (abstract.length > 500 ? '...' : ''),
          url: paperUrl,
          company: 'DeepMind',
          tags: generateTags(title + ' ' + abstract + ' deepmind')
        })
        
        console.log(`üìù Found paper: "${title}" (${publication_date})`)
      }
    })
    
    // If still no papers, try the original selector approach
    if (papers.length === 0) {
      console.log('‚ö†Ô∏è No papers found with card approach, trying selectors...')
      
      for (const selector of selectors) {
        $(selector).each((_, element) => {
          const $elem = $(element)
        
        // Extract title
        let title = $elem.find('h2, h3, h4').first().text().trim() ||
                   $elem.find('[class*="title"]').first().text().trim() ||
                   $elem.find('a').first().text().trim()
        
        // Extract URL
        let paperUrl = $elem.find('a[href*="/research/publications/"]').first().attr('href') ||
                      $elem.find('a').first().attr('href') ||
                      $elem.attr('href') || ''
        
        // Skip if we've already seen this title or URL
        if (!title || foundTitles.has(title) || (paperUrl && foundUrls.has(paperUrl))) {
          return
        }
        
        // Skip navigation elements and page headers
        if (title.length < 10 || title.length > 300 || 
            /^(Research|Publications|About|Blog|Careers)$/i.test(title) ||
            title.includes('Explore a selection of') ||
            title.includes('\n')) {
          return
        }
        
        foundTitles.add(title)
        if (paperUrl) foundUrls.add(paperUrl)
        
        // Normalize URL
        if (paperUrl && !paperUrl.startsWith('http')) {
          paperUrl = `https://deepmind.google${paperUrl.startsWith('/') ? '' : '/'}${paperUrl}`
        }
        
        // Extract date
        const dateText = $elem.find('[class*="date"], time').first().text().trim() ||
                        $elem.text()
        
        let publication_date = '2024-01-01'
        const dateMatch = dateText.match(/\b(202[0-9]|201[0-9])-(\d{1,2})-(\d{1,2})\b/) ||
                         dateText.match(/\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{1,2}),?\s+(202[0-9]|201[0-9])\b/i) ||
                         dateText.match(/\b(202[0-9]|201[0-9])\b/)
        
        if (dateMatch) {
          try {
            const dateStr = dateMatch[0]
            const date = new Date(dateStr)
            if (!isNaN(date.getTime())) {
              publication_date = date.toISOString().split('T')[0]
            }
          } catch (e) {
            // Keep default date
          }
        }
        
        // Extract abstract/description
        const abstract = $elem.find('[class*="description"], [class*="abstract"], p').first().text().trim() ||
                        $elem.find('p').first().text().trim() ||
                        'Research publication from DeepMind.'
        
        // Extract authors if available
        const authorsText = $elem.find('[class*="author"]').text().trim()
        const authors = authorsText ? authorsText.split(/[,;]/).map(a => a.trim()).filter(a => a) : ['DeepMind']
        
        papers.push({
          title,
          authors,
          publication_date,
          abstract: abstract.substring(0, 500) + (abstract.length > 500 ? '...' : ''),
          url: paperUrl || `https://deepmind.google/research/publications/`,
          company: 'DeepMind',
          tags: generateTags(title + ' ' + abstract + ' deepmind')
        })
        
        console.log(`üìù Found paper: "${title}" (${publication_date})`)
        })
      }
    }
    
    // Also try to extract papers from any JSON-LD structured data
    const jsonLdScripts = $('script[type="application/ld+json"]').toArray()
    for (const script of jsonLdScripts) {
      try {
        const data = JSON.parse($(script).html() || '{}')
        if (data['@type'] === 'ScholarlyArticle' || data['@type'] === 'Article') {
          const title = data.headline || data.name
          if (title && !foundTitles.has(title)) {
            foundTitles.add(title)
            papers.push({
              title,
              authors: data.author ? (Array.isArray(data.author) ? data.author.map((a: any) => a.name || a) : [data.author.name || data.author]) : ['DeepMind'],
              publication_date: data.datePublished || '2024-01-01',
              abstract: data.description || 'Research publication from DeepMind.',
              url: data.url || `https://deepmind.google/research/publications/`,
              company: 'DeepMind',
              tags: generateTags(title + ' ' + (data.description || '') + ' deepmind')
            })
            console.log(`üìù Found paper from JSON-LD: "${title}"`)
          }
        }
      } catch (e) {
        // Ignore JSON parsing errors
      }
    }
    
    console.log(`‚úÖ Found ${papers.length} papers total from DeepMind`)
    
    // If we still have no papers, try one more approach using page evaluation
    if (papers.length === 0) {
      console.log('‚ö†Ô∏è No papers found via HTML parsing, trying page evaluation...')
      
      const evaluatedPapers = await page.evaluate(() => {
        const links = Array.from(document.querySelectorAll('a[href*="/research/publications/"]'))
        return links
          .filter(link => {
            const href = link.getAttribute('href') || ''
            const text = link.textContent?.trim() || ''
            return href.match(/\/research\/publications\/[a-z0-9-]+$/i) && 
                   text.length > 10 && 
                   text.length < 300
          })
          .map(link => ({
            title: link.textContent?.trim() || '',
            url: link.getAttribute('href') || '',
            parentText: link.closest('article, div, li')?.textContent?.substring(0, 200) || ''
          }))
      })
      
      console.log(`üìä Found ${evaluatedPapers.length} papers via page evaluation`)
      
      evaluatedPapers.forEach(paper => {
        if (paper.title && !foundTitles.has(paper.title)) {
          const url = paper.url.startsWith('http') ? paper.url : `https://deepmind.google${paper.url}`
          
          let publication_date = '2024-01-01'
          const dateMatch = paper.parentText.match(/\b(202[0-9]|201[0-9])\b/)
          if (dateMatch) {
            publication_date = `${dateMatch[0]}-01-01`
          }
          
          papers.push({
            title: paper.title,
            authors: ['DeepMind'],
            publication_date,
            abstract: 'Research publication from DeepMind.',
            url,
            company: 'DeepMind',
            tags: generateTags(paper.title + ' deepmind')
          })
          
          console.log(`üìù Found paper via evaluation: "${paper.title}"`)
        }
      })
    }
    
    console.log(`‚úÖ Found ${papers.length} papers total from DeepMind`);
    
  } catch (error) {
    console.error('Error scraping DeepMind with Puppeteer:', error);
  }
}

function generateTags(text: string): string[] {
  const tags: string[] = [];
  const lowerText = text.toLowerCase();
  
  // AI/ML keywords
  const keywords = [
    // Models
    'gpt-5', 'gpt-4', 'gpt-oss', 'chatgpt', 'dall-e', 'whisper', 'sora', 'clip', 'codex', 'o1', 'o3', 'o4',
    // Companies
    'openai', 'anthropic', 'deepmind', 'gemini', 'alphafold',
    // Technical terms
    'transformer', 'attention', 'llm', 'language model', 'neural network',
    // Safety and alignment
    'safety', 'alignment', 'rlhf', 'constitutional ai', 'robustness',
    // Training and optimization
    'scaling', 'fine-tuning', 'training', 'inference', 'optimization', 'reinforcement learning',
    // Capabilities
    'multimodal', 'vision', 'reasoning', 'benchmark', 'evaluation', 'generalization',
    // Research areas
    'interpretability', 'adversarial', 'few-shot', 'zero-shot', 'meta-learning',
    // Application areas
    'healthcare', 'clinical', 'copilot', 'agent', 'robotics', 'game playing'
  ];
  
  keywords.forEach(keyword => {
    if (lowerText.includes(keyword)) {
      tags.push(keyword.replace(/\s+/g, '-'));
    }
  });
  
  return [...new Set(tags)]; // Remove duplicates
}